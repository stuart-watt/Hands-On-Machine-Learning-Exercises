{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 Ensemble Learning and Random Forests\n",
    "\n",
    "In this notebook we will go through ensemble learning methods. An *ensemble* is a group of predictors (classifiers or regressors) whose aggregated prediction is better than the prediction of each individual predictor. This is called the *wisdom of the crowd*.\n",
    "\n",
    "At the end of a project you will often create an ensemble from all the different well-performing models you built and tested along the way. This is a way of making your final model as good as possible. Instead of relying on one good model, use the aggregated prediction from a few good mdels that differ in their methods and such.\n",
    "\n",
    "## Voting classifiers\n",
    "\n",
    "If you have built a few different classifier models (logistic regression clf, decision tree clf, svm clf etc) on a particular dataset that all perform reasonably well (around 80%), a simple way to get an even better classifier is to aggregate the predictions of each classifier and predict the class with the most votes. This majority-vote classifier is a *hard-voting* classifier.\n",
    "\n",
    "Even if the models are *weak learners* (predicting slightly better than just guessing), the ensemble can be a *strong learner* (high accuracy) due to the *law of large numbers*. Provided there are enough contributing models.\n",
    "\n",
    "The key thing to consider is that the models should be as independent from each otehr as possible (eg different training algorithms or different models all together) so that the models don't all make the same types of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)), ('rf', RandomF...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "forest_clf = RandomForestClassifier(n_estimators=10)\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', forest_clf), ('svc', svm_clf)],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.85\n",
      "RandomForestClassifier 0.87\n",
      "SVC 0.87\n",
      "VotingClassifier 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\21493806\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for clf in(log_clf, forest_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the classifier which takes the vote between all three classifiers performs better than each individual classifier.\n",
    "\n",
    "*Soft voting* is when the ensemble model predicts the class which has the highest probability averaged over all the individual models. In this case, the models need to have a predict_proba() method. This type of voting gives higher weight to more confident models and so often gives better accuracy than hard voting. Lets do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.85\n",
      "RandomForestClassifier 0.88\n",
      "SVC 0.87\n",
      "VotingClassifier 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\21493806\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression()\n",
    "forest_clf = RandomForestClassifier()\n",
    "# by default the SVC class does not make predictions based on probabilities. Setting the probability\n",
    "# hyperparameter to True makes the class use cross validation to calculate probailities for classes.\n",
    "svm_clf = SVC(probability=True)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', forest_clf), ('svc', svm_clf)],\n",
    "    # Simply change the voting hyperparameter to 'soft'\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "for clf in(log_clf, forest_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset, all the models are performing well, so it is hard to see the benefit of the voting classifier.\n",
    "\n",
    "## Bagging and Pasting\n",
    "\n",
    "In order to get a diverse ensemble, you can either train the models using very different algorithms, or to use the same algorithms but train them on different random subsets of the training set.\n",
    "\n",
    "In *bagging* (short for *bootstrap aggregating*), the sampling is performed *with* replacement (called 'bootstrapping' in statistics). This means that an training instance can be sampled again for the same predictor.\n",
    "\n",
    "In *pasting*, the sampling is performed *without* replacement. So a predictor can only be trained by a particular instance once.\n",
    "\n",
    "The final prediction is typically the *statistical mode* (hard voting) for classifiers or the average for regression. Aggregation reduces both bias and variance.\n",
    "\n",
    "Bagging and pasting scale well with the size of the problem since the individual models can be trained and evaluated in parallel (multiple cores or servers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
